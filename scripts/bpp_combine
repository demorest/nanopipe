#! /usr/bin/env python

# combine BPP individual raw files into single PSRFITS output

import os, sys
import shutil
import logging
import psrchive
from operator import attrgetter
from nanopipe import psrindex
from nanopipe.get_proper_name import proper_name
import numpy as np
from scipy.interpolate import InterpolatedUnivariateSpline

logging.basicConfig(format="%(asctime)-15s "
        +" %(levelname)8s %(message)s", level=logging.INFO)

be = 'ABPP'

dry = False

# Reads index.db in current dir
db = psrindex.PSRIndex()
files = db.select(where='backend="%s"' % be)

logging.info('Processing %d %s files from \'%s\'' 
        % (len(files), be, db.dbfilename))

def bpp_idx(fname): 
    # Return the file index assuming c*.dat naming
    if not (fname.startswith('c') and fname.endswith('.dat')):
        raise RuntimeError('bpp_idx: invalid filename (\'%s\')' % fname)
    return int(fname[1:7])

def check_match(arch1, arch2):
    # Check if two files are likely to be from the same scan
    # Things that needs to match exactly:
    match_funcs = [
            psrchive.Archive.get_source,
            psrchive.Archive.get_npol,
            psrchive.Archive.get_nchan,
            psrchive.Archive.get_type,
            psrchive.Archive.get_telescope,
            psrchive.Archive.get_backend_name,
            psrchive.Archive.get_receiver_name,
            ]
    for fn in match_funcs:
        if fn(arch1) != fn(arch2):
            logging.debug('Found %s difference' % fn.__str__())
            return False
    # Things that should match to some tolerance
    match_tol = [
            (psrchive.Archive.get_centre_frequency, 1e-6),
            (psrchive.Archive.get_bandwidth, 1e-2),
            (lambda x: x[0].get_duration(), 1e-3),
            ]
    for (fn, tol) in match_tol:
        if abs(fn(arch1)-fn(arch2))/fn(arch1) > tol:
            logging.debug('Found %s difference' % fn.__str__())
            return False
    # Large time difference
    #time_diff = abs((arch2[0].get_epoch() - arch1[0].get_epoch()).in_seconds())
    #if time_diff > (arch1[0].get_duration()*2.0):
    #    logging.debug('Found time difference (dt = %.3f s)' % time_diff)
    #    return False
    # All checks pass, return True
    return True

def midscan_correction(archive):
    # After merging files, call this to do the midscan correction.
    nsub = archive.get_nsubint()
    t0 = archive[0].get_epoch()

    # If only one subint, can't do much:
    if nsub==1:
        p = archive[0].get_folding_period()
        d = archive[0].get_duration()
        dphs = np.fmod(0.5*d, p)
        ep = t0 + 0.5*d - dphs*p
        archive[0].set_epoch(ep)
        return

    # Otherwise, do it
    else:

        t = np.array([(i.get_epoch() - t0).in_seconds() for i in archive])
        f = np.array([1.0/i.get_folding_period() for i in archive])
        d = np.array([i.get_duration() for i in archive])

        spl_k = min(nsub-1,3)

        spl = InterpolatedUnivariateSpline(t + 0.5*d, f, k=spl_k,
                bbox=[0.0,t[-1]+d[-1]])

        for i in range(nsub):
            dphs = np.fmod(spl.integral(t[i],t[i]+d[i])/2.0,1.0)
            dphs0 = np.fmod(0.5*d[i]*f[i],1.0)
            logging.debug('dphs = %.3e' % (dphs-dphs0))
            ep = archive[i].get_epoch() + 0.5*d[i] - dphs/f[i]
            archive[i].set_epoch(ep)

# Set up TimeAppend
timeappend = psrchive.TimeAppend()
timeappend.chronological = True

# Output dir
tmpdir = '/dev/shm'
outdir = './merged'

current_files = []
current_archs = []
for f in sorted(files,key=attrgetter('fname')):

    logging.debug('Processing ' + f.full_path)

    # Load each file, either add it to the current set or
    # unload the current set and start a new one
    try:
        arch = psrchive.Archive_load(str(f.full_path))
    except KeyboardInterrupt:
        logging.info('Caught Ctrl-C, exiting.')
        break
    except:
        logging.warn('Error loading ' + f.full_path + ', skipping')
        continue

    # Test for bad files here
    if arch[0].get_duration()==0.0: 
        logging.info('Skipping zero-length file \'%s\'' % f.fname)
        continue

    # Default to not unload
    do_unload = False

    # non-empty current set, need to check for unload
    if len(current_files):
        if bpp_idx(f.fname)-bpp_idx(current_files[-1].fname) > 1:
            logging.debug('Index difference > 1')
            do_unload = True
        elif not check_match(current_archs[-1], arch):
            do_unload = True

    # Unload if needed
    if do_unload:
        arch0 = current_archs[0]
        outfname = '%s_%05d_c%06d_c%06d_%s.ar' % (be,
                int(arch0[0].get_epoch().in_days()),
                bpp_idx(current_files[0].fname),
                bpp_idx(current_files[-1].fname),
                arch0.get_source())
        logging.info('Unloading \'%s\' (nsub=%d)' % (outfname,
            len(current_archs)))
        timeappend.init(arch0)
        for a in current_archs[1:]: timeappend.append(arch0,a)
        midscan_correction(arch0)
        if not dry:
            tmpout = os.path.join(tmpdir,outfname)
            arch0.unload(tmpout)
            shutil.move(tmpout,os.path.join(outdir,outfname))

        current_files = []
        current_archs = []

    # Add new file to list
    current_files.append(f)
    current_archs.append(arch)



